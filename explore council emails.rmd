---
title: "What do Boulderites Care About?"
subtitle: "An Exploratory Analysis into City Council Emails"
author: "Eric Stern"
date: "7/7/2021"
output: html_document
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(tidytext)
library(wordcloud)
library(topicmodels)
library(quanteda)
library(plotly)
library(kableExtra) #create attractive tables
library(ggrepel) #text and label geoms for ggplot2
library(formattable)
library(rsample)
library(future)
library(furrr)

# functions
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

format_table <- function(data) {
  data %>% 
    rename_all(~str_to_title(.) %>% 
                 str_replace_all("_", " ") %>% 
                 str_wrap(width = 30)) %>% 
    knitr::kable()
}


word_chart <- function(data, input, title) {
  data %>%
  #set y = 1 to just plot one variable and use word as the label
  ggplot(aes(as.factor(row), 1, label = input, fill = factor(topic) )) +
  #you want the words, not the points
  geom_point(color = "transparent") +
  #make sure the labels don't overlap
  geom_label_repel(nudge_x = .2,  
                   direction = "y",
                   box.padding = 0.1,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~topic) +
    theme_dark() +
    theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
          #axis.title.x = element_text(size = 9),
          panel.grid = element_blank(), panel.background = element_blank(),
          panel.border = element_rect("lightgray", fill = NA),
          strip.text.x = element_text(size = 9)) +
    labs(x = NULL, y = NULL, title = title) 
}
```

# About the Data

This is data that the city of Boulder keeps open to the public. They maintain \~30 data sets for various different areas of interest such as crime, homelessness, and city affairs. This analysis is leveraging the data set, [2019 Council Emails Dataset](https://bouldercolorado.gov/open-data/emails-to-boulder-city-council/):

> Data set containing 2019 emails to [council\@bouldercolorado.gov](mailto:council@bouldercolorado.gov){.email}. This version works best for viewing in Excel, as it includes only the plain text version of the emails. This file is updated daily with new emails.

```{r}
raw <- list()
raw$emails_2019 <- 
  read_csv(
    "http://www-static.bouldercolorado.gov/docs/opendata/CouncilEmails_PlainText2019.csv",
    col_types = cols(.default = col_character()))

raw$emails_2020 <- 
  read_csv(
    "http://www-static.bouldercolorado.gov/docs/opendata/CouncilEmails_PlainText2020.csv",
    col_types = cols(.default = col_character()))

raw_data <-
  bind_rows(raw)

```

# Understanding the Data

This data set has the following columns : `r names(raw_data)`. They are relatively self explanatory. One item to note is that all emails included in this data set are sent to [council\@bouldercolorado.gov](mailto:council@bouldercolorado.gov){.email}. They may also be sent to others such as boulderplanningboard or specific city council members.

# Data Cleaning

These data are a little messy. Not bad in the grand scheme of data messiness but we need to remove some junk if we want any reasonable results. I have created a table of what I removed and why:

```{r}
removed <- 
  list()
removed$no_reply <- 
  raw_data %>% 
  filter(SentFrom == "No Reply") # these are spam

removed_rows <- 
  bind_rows(removed, .id = "removal_reason")

city_council_members <- 
  c("Jones, Suzanne",
    "Weaver,  Sam",
    "Brockett, Aaron",
    "Yates,  Phillip",
    "Carlisle, Cynthia",
    "Grano, Jill",
    "Morzel, Lisa",
    "Young,  Mary",
    "Nagle, Mirabai")

city_officials <- 
  c("Aulabaugh, Shannon", # police spokesperson
    "Brautigam, Jane") #city manager 

data_clean <- 
  raw_data %>% 
  # remove bad data
  anti_join(removed_rows %>% 
              select(-removal_reason)) %>% 
  # create some flag variables
  mutate(
    IsMasked = ifelse(str_detect(SentFrom, "\\[*\\]"), T, F),
    IsReply  = ifelse(str_detect(EmailSubject, "^R[E,e]:"), T, F),
    FromCCM  = ifelse(str_detect(SentFrom, 
                                 pattern = str_c(city_council_members, collapse = "|")),
                      T,
                      F),
    ToCCM  = ifelse(str_detect(SentTo, 
                               pattern =  str_c(city_council_members, collapse = "|")),
                    T,
                    F),
    FromCO = ifelse(str_detect(SentFrom, 
                               pattern =  str_c(city_officials, collapse = "|")),
                    T,
                    F)
    ) %>% 
  # format strings
  mutate_at(vars(EmailSubject), str_to_lower) %>% 
  mutate(ReceivedDate = as_date(ReceivedDate),
         full_email = paste(EmailSubject, PlainTextBody)) # include subject as not all emails reiterate their subject in the body. 
  


removed_rows %>% 
  group_by(removal_reason) %>% 
  summarise(n_rows_removed = n()) %>% 
  ungroup() %>% 
  format_table()
```

## Metadata

The meat of this data set is the plain email text body. We'll get to that later. First, let's look at the meta data about whose sending the emails and when they are sending them.

### Senders

My hypothesis is that there aren't too many unique senders. Unfortunately public engagement is low in general. As expected, we see a small handful having set the majority of emails. As a note, the largest correspondent is the spokesperson for the Boulder police department.

```{r}
frequency_email <- data_clean %>% 
  count(SentFrom, FromCCM, FromCO) %>% 
  mutate(FromCCM = case_when(FromCCM ~ "City Council Member", 
                             FromCO  ~ "City Official",
                             T       ~ "Constituent")) %>% 
  arrange(desc(n)) %>% 
  mutate(SentFrom = as_factor(SentFrom)) 

frequency_email %>% 
  plot_ly(x = ~SentFrom, y = ~n, color = ~FromCCM, type = "bar")
```

If we look at some descriptive statistics about whose sending emails and their frequency, we get the following:

```{r}


frequency_email %>% 
  summarise(total_observations = sum(n),
            mean   = mean(n),
            median = median(n),
            mode   = getmode(n),
            max    = max(n),
            min    = min(n)) %>% 
 format_table()
```

As expected, this is a very skewed distribution with most only sending one email and one individual having sent `r pluck(summarise(frequency_email, max = max(n)), 1)`.

## Dates of Emails

Looking at the dates that emails get sent, it appears that there are outside triggers that incentive people to send emails to the city council on certain days. Without looking at the calendar for city council sessions, it is a good guess that the days with the most email traffic are days prior and post city council sessions.

*edit* Since initially performing this analysis, the COVID-19 Pandemic has swept the globe and thrown data projects everywhere into disarray. It is somewhat apparent that engagement has increased in a large way.

```{r}
data_clean %>% 
  count(ReceivedDate) %>% 
  plot_ly(x = ~ReceivedDate, y = ~n, type = 'scatter', mode = 'lines')
  
```

## Council and City Officials Engagement

Based on reply signs in the email subject, we can also see how engaged the council or city officials are with constituents:

```{r}
data_clean %>% 
  count(IsReply) %>% 
  filter(!is.na(IsReply)) %>% 
  mutate(IsReply = ifelse(IsReply, "Response Email", "Original Email")) %>% 
  plot_ly(x = ~IsReply, y = ~n, type = "bar")
```

This is a bit low, I would expect more engagement since most emails are relatively unique and not campaigns by interest groups. Let's see what types of trends we can find based on when city officials reply.

## Email Duplicates

People frequently copy paste an email in order to show more public awareness to an issue. We are checking by email subject for now. As expected, there are some movements in Boulder that coordinate the efforts through this email channel. Since people tend to sign their own name, we took the email subject to look for duplicates. This isn't perfect but does a good job at clearing out the vast majority.

*Edit* Since performing this analysis, the largest movement towards racial equality has begun. This significantly changes my initial findings.

```{r}

data_clean %>% 
  filter(!IsReply) %>% 
  count(EmailSubject) %>% 
  arrange(desc(n)) %>% 
  mutate(EmailSubject = as_factor(EmailSubject)) %>% 
  plot_ly(x = ~EmailSubject, y = ~n, type = "bar")

```

We are starting to see some trends here. This is not conclusive, but gives us a good starting point for further analysis. There are a couple of notable trends. Mostly surrounding Boulder's Open Space Mountain Parks, Bike Trails, Vaping, Homeless Shelters, 5g, and very notably, Police Accountability:

```{r}
data_clean %>% 
  filter(!IsReply) %>% 
  count(EmailSubject) %>% 
  arrange(desc(n)) %>% 
  head(10) %>% 
  format_table()
```

This is a good start. Let's first bin the emails into categories by extracting common phrases

## Find most commons words and phrases

```{r}
tidy_text <- data_clean %>% 
  unnest_tokens(word, EmailSubject) %>% 
  anti_join(stop_words, by = "word")
  

tidy_text %>% 
  count(word) %>% 
  filter(!(word %in% c("boulder", "council", "city", 'fw', "fwd", '2019'))) %>% # remove some meaningless words
  with(wordcloud(word, n, max.words = 50))


tidy_phrases <- 
  data_clean %>% 
  unnest_tokens(word, PlainTextBody, token = "ngrams", n = 2) %>% 
  separate(word, into = c("w_1", "w_2"), remove = F) %>% 
  anti_join(stop_words, by = c("w_1" = "word")) %>% 
  anti_join(stop_words, by = c("w_2" = "word")) %>% 
  count(word) %>% 
  arrange(desc(n))

tidy_phrases
```

# Text Analysis

For this iteration, we are using a simple "bag of words" approach using Latent Dirichlet Allocation. This is a relatively simplistic language model that builds off of other Bayesian methods. Like other bag of words approaches, the model does not conflate location in a string with significance and is purely based off of frequency.

```{r}
# create DTM with the cleaned data
# clean_dfm <- data_clean %>% 
#   mutate(PlainTextBody = str_replace_all(PlainTextBody, '\\[.+\\]', ' ')) %>% 
#   filter(!is.na(PlainTextBody)) %>% 
#   corpus(docid_field = 'MessageIdentifier',
#          text_field = 'PlainTextBody') %>% 
#   dfm(tolower = TRUE, 
#       stem = TRUE,
#       remove_punct = TRUE, 
#       remove_symbols = TRUE, 
#       remove_numbers = TRUE, 
#       remove = stopwords('en')) %>% 
#   dfm_trim(min_termfreq = 0.95, termfreq_type = "quantile", 
#            max_docfreq = 0.1, docfreq_type = "prop") %>% 
#   convert(to = "topicmodels")

# oddities <- data_clean %>% 
#   unnest_tokens(word, full_email) %>% 
#   filter(!str_detect(word, '[0-9,a-z]+'))

clean_unnest <- data_clean %>% 
  unnest_tokens(word, full_email) %>% 
  anti_join(stop_words, by = c("word")) %>% 
  filter(!str_detect(word, '[0-9]+'),
         !(word %in% c('http' , 'https', 'tinurl\\.com')),
         str_detect(word, '[a-z]+')) %>% # have some issues with non-ASCII characters causing problems
  mutate(word_cleaned = textstem::lemmatize_words(word)) %>%
  count(word_cleaned, MessageIdentifier)

```

Choose K by looking at perplexity plots

```{r eval=FALSE, include=FALSE}
plan('multicore')

split_dat <- clean_unnest %>% 
  nest(-MessageIdentifier) %>% 
  sample_n(100) %>% 
  initial_split()

train_lda <- function(k, dat) {
  mod <- LDA(cast_dtm(training(dat) %>% 
                        unnest(data), 
                      term = word_cleaned,
                      document = MessageIdentifier,
                      value = n),
             k = k, 
             method = "GIBBS")
  
  perp <- perplexity(mod, 
                     cast_dtm(testing(dat)%>% 
                               unnest(data), 
                             term = word_cleaned,
                             document = MessageIdentifier,
                             value = n))
  
  return(perp)
}

n_topics = seq(10, 60, by = 2)


model_perplexities <- 
  future_map(n_topics,
    train_lda, 
    dat = split_dat,
    .options = furrr_options(seed = TRUE))

ggplot() + 
  aes(x = n_topics,
      y = unlist(model_perplexities)) +
  geom_point()
```

The perplexity plots I built showed that 35-45 topics has the best fit. This is similar to an 'elbow plot' we would make if using K-means or other unsupervised approaches. Ideally we would choose a topic based on a known understanding of the data, but since it isn't always as clear, we pick N where the curve starts to flatten. 30 to 40 topics makes sense as there are a lot of issues and they are changing over time. However this is very hard to build a story around. Since we have no clear topics in mind, reducing that number will help down the road.

```{r}
k <- 30 #number of topics

seed = 1234 #necessary for reproducibility
#fit the model passing the parameters discussed above
#you could have more control parameters but will just use seed here
lda <-
  clean_unnest %>% 
  cast_dtm(document = MessageIdentifier,
           term = word_cleaned,
           value = n) %>% 
  LDA(k = k, 
      method = "GIBBS", 
      control = list(seed = seed))



num_words <- 10 #number of words to visualize

#create function that accepts the lda model and num word to display
top_terms_per_topic <- function(lda_model, num_words) {

  #tidy LDA object to get word, topic, and probability (beta)
  topics_tidy <- tidy(lda_model, matrix = "beta")


  top_terms <- topics_tidy %>%
  group_by(topic) %>%
  arrange(topic, desc(beta)) %>%
  #get the top num_words PER topic
  slice(seq_len(num_words)) %>%
  arrange(topic, beta) %>%
  #row is required for the word_chart() function
  mutate(row = row_number()) %>%
  ungroup() %>%
  #add the word Topic to the topic labels
  mutate(topic = paste("Topic", topic, sep = " "))
  #create a title to pass to word_chart
  # title <- paste("LDA Top Terms for", k, "Topics")
  # #call the word_chart function you built in prep work
  # word_chart(top_terms, top_terms$term, title)
  
  return(top_terms)
}
#
top_terms_per_topic(lda, num_words)
```

# Assign topic to each email

```{r}
topic_designation <- 
  tidy(lda, 'gamma') 


topic_words <- 
  tidy(lda, 'beta') %>% 
  arrange(desc(beta))
# topic_designation %>% 
#   filter(document == unique(.$document)[5007]) %>% 
#   plot_ly() %>% 
#   add_pie(values = ~gamma,
#           labels = ~ topic)
topic_labels <- topic_words %>% 
  group_by(topic) %>% 
  filter(beta == max(beta)) %>% 
  mutate(term = str_to_title(term)) %>% 
  ungroup()


emails_topics <- data_clean %>% 
  left_join(topic_designation,
            by = c('MessageIdentifier' = 'document')) %>% 
  left_join(topic_labels, by = 'topic')


```

# Plot topics overtime

With the unrest in 2020, it is likely that topics were assigned with temporal correlation. The plot below showcases the topics over time.

```{r}
emails_topics %>% 
  group_by(MessageIdentifier) %>% 
  filter(gamma == max(gamma)) %>% 
  mutate(topic = paste("Topic", 
                       topic)) %>% 
  ggplot() +
  aes(x = ReceivedDate,
      fill = topic) +
  geom_density(alpha = .5)
```

```{r}
write_rds(emails_topics,
          'local_politics/clustered_assignments.rds')

write_rds(lda,
          'local_politics/topic_model.rds')
```
