---
title: "What do Boulderites Care About?"
subtitle: "An Exploratory Analysis into City Council Emails"
author: "Eric Stern"
date: "8/31/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidygraph)
library(ggraph)
library(lubridate)
library(tidytext)
library(wordcloud)
library(topicmodels)
library(plotly)
library(DT)
library(kableExtra) #create attractive tables
library(ggrepel) #text and label geoms for ggplot2
library(gridExtra)
library(formattable)

# functions
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}

format_table <- function(data) {
  data %>% 
    rename_all(~str_to_title(.) %>% 
                 str_replace_all("_", " ") %>% 
                 str_wrap(width = 30)) %>% 
    knitr::kable()
}


word_chart <- function(data, input, title) {
  data %>%
  #set y = 1 to just plot one variable and use word as the label
  ggplot(aes(as.factor(row), 1, label = input, fill = factor(topic) )) +
  #you want the words, not the points
  geom_point(color = "transparent") +
  #make sure the labels don't overlap
  geom_label_repel(nudge_x = .2,  
                   direction = "y",
                   box.padding = 0.1,
                   segment.color = "transparent",
                   size = 3) +
  facet_grid(~topic) +
    theme_dark() +
    theme(axis.text.y = element_blank(), axis.text.x = element_blank(),
          #axis.title.x = element_text(size = 9),
          panel.grid = element_blank(), panel.background = element_blank(),
          panel.border = element_rect("lightgray", fill = NA),
          strip.text.x = element_text(size = 9)) +
    labs(x = NULL, y = NULL, title = title) +
    #xlab(NULL) + ylab(NULL) +
  #ggtitle(title) +
  coord_flip()
}
```

# About the Data
This is data that the city of Boulder keeps open to the public. They maintain ~30 data sets for various different areas of interest such as crime, homelessness, and city affairs. This analysis is leveraging the data set, [2019 Council Emails Dataset](https://bouldercolorado.gov/open-data/emails-to-boulder-city-council/):

>Data set containing 2019 emails to council@bouldercolorado.gov. This version works best for viewing in Excel, as it includes only the plain text version of the emails. This file is updated daily with new emails.

```{r}
raw_data <- 
  read_csv("http://www-static.bouldercolorado.gov/docs/opendata/CouncilEmails_PlainText2019.csv")

# raw_data %>% 
#   datatable()
```

# Understanding the Data
This data set has the following columns : `r names(raw_data)`. They are relatively self explanatory. One item to note is that all emails included in this data set are sent to council@bouldercolorado.gov. They may also be sent to others such as boulderplanningboard or specific city council members.

# Data Cleaning
These data are a little messy. Not bad in the grand scheme of data messiness but we need to remove some junk if we want any reasonable results. I have created a table of what I removed and why:
```{r}
removed <- 
  list()
removed$no_reply <- 
  raw_data %>% 
  filter(SentFrom == "No Reply") # these are spam

removed_rows <- 
  bind_rows(removed, .id = "removal_reason")

city_council_members <- 
  c("Jones, Suzanne",
    "Weaver,  Sam",
    "Brockett, Aaron",
    "Yates,  Phillip",
    "Carlisle, Cynthia",
    "Grano, Jill",
    "Morzel, Lisa",
    "Young,  Mary",
    "Nagle, Mirabai")

city_officials <- 
  c("Aulabaugh, Shannon", # police spokesperson
    "Brautigam, Jane") #city manager 

data_clean <- 
  raw_data %>% 
  # remove bad data
  anti_join(removed_rows %>% 
              select(-removal_reason)) %>% 
  # create some flag variables
  mutate(IsMasked = ifelse(str_detect(SentFrom, "\\[*\\]"), T, F),
         IsReply  = ifelse(str_detect(EmailSubject, "^R[E,e]:"), T, F),
         FromCCM  = ifelse(str_detect(SentFrom, 
                                      pattern = str_c(city_council_members, collapse = "|")),
                           T,
                           F),
         ToCCM  = ifelse(str_detect(SentTo, 
                             pattern =  str_c(city_council_members, collapse = "|")),
                           T,
                           F),
         FromCO = ifelse(str_detect(SentFrom, 
                             pattern =  str_c(city_officials, collapse = "|")),
                           T,
                           F)) %>% 
  # format strings
  mutate_at(vars(EmailSubject), str_to_lower) %>% 
  mutate(ReceivedDate = as_date(ReceivedDate)) 
  


removed_rows %>% 
  group_by(removal_reason) %>% 
  summarise(n_rows_removed = n()) %>% 
  ungroup() %>% 
  format_table()
```


## Metadata 
The meat of this data set is the plain email text body. We'll get to that later. First, let's look at the meta data about whose sending the emails and when they are sending them.

### Senders
My hypothesis is that there aren't too many unique senders. Unfortunately public engagement is low in general. As expected, we see a small handful having set the majority of emails. As a note, the largest corresponder is the spokeperson for the Boulder police department.
```{r}
frequency_email <- data_clean %>% 
  count(SentFrom, FromCCM, FromCO) %>% 
  mutate(FromCCM = case_when(FromCCM ~ "City Council Member", 
                             FromCO  ~ "City Official",
                             T       ~ "Constituent")) %>% 
  arrange(desc(n)) %>% 
  mutate(SentFrom = as_factor(SentFrom)) 

frequency_email %>% 
  plot_ly(x = ~SentFrom, y = ~n, color = ~FromCCM, type = "bar")
```

If we look at some descriptive statistics about whose sending emails and their frequency, we get the following:
```{r}


frequency_email %>% 
  summarise(total_observations = sum(n),
            mean   = mean(n),
            median = median(n),
            mode   = getmode(n),
            max    = max(n),
            min    = min(n)) %>% 
 format_table()
```

As expected, this is a very skewed distribution with most only sending one email and one individual having sent `r pluck(summarise(frequency_email, max = max(n)), 1)`.

## Dates of Emails
Looking at the dates that emails get sent, it appears that there are outside triggers that incent people to send emails to the city council on certain days. Without looking at the calandar for city council sessions, it is a good guess that the days with the most email traffic are days prior and post city council sessions.
```{r}
data_clean %>% 
  count(ReceivedDate) %>% 
  plot_ly(x = ~ReceivedDate, y = ~n, type = 'scatter', mode = 'lines')
  
```

## Coucil and City Officials Engagement
Based on reply signs in the email subject, we can also see how engaged the council or city officials are with constituents:

```{r}
data_clean %>% 
  count(IsReply) %>% 
  filter(!is.na(IsReply)) %>% 
  mutate(IsReply = ifelse(IsReply, "Response Email", "Original Email")) %>% 
  plot_ly(x = ~IsReply, y = ~n, type = "bar")
```

This is a bit low, I would expect more engagement since most emails are relatively unique and not campaigns by interest groups. Let's see what types of trends we can find based on when city officials reply.

## Network Effects

As there are a lot of people in the CC line of these emails, we can also see what the network looks like using some graph theory.

```{r}
  data_clean %>% 
  select(SentFrom, SentTo) %>% 
  separate_rows(SentTo, sep = ",") %>% 
  filter(str_detect(SentTo, "\\[.*\\]")) %>% 
  mutate(SentTo = str_trim(SentTo, side = "both")) %>% 
  as_tbl_graph() %>% 
  ggraph(layout = 'kk') +
  geom_edge_link() +
  geom_node_point() +
  theme_light()
```

## Email Duplicates
People frequently copy paste an email in order to show more public awareness to an issue. We are checking by email subject for now. As expected, there are some movements in Boulder that coordinate the efforts through this email channel. Since people tend to sign their own name, we took the email subject to look for duplicates. This isn't perfect but does a good job at clearing out the vast majority.
```{r}

data_clean %>% 
  filter(!IsReply) %>% 
  count(EmailSubject) %>% 
  arrange(desc(n)) %>% 
  mutate(EmailSubject = as_factor(EmailSubject)) %>% 
  plot_ly(x = ~EmailSubject, y = ~n, type = "bar")

```

We are starting to see some trends here. This is not conclusive, but gives us a good starting point for further analysis. There are a couple of notable trends. Mostly surounding Boulder's Open Space Mountain Parks, Bike Trails, Vaping, Homeless Shelters, and 5g:
```{r}
data_clean %>% 
  filter(!IsReply) %>% 
  count(EmailSubject) %>% 
  arrange(desc(n)) %>% 
  head(10) %>% 
  format_table()
```

This is a good start. Let's first bin the emails into categories by extracting common phrases

## Find most commons words and phrases
```{r}
tidy_text <- data_clean %>% 
  unnest_tokens(word, EmailSubject) %>% 
  anti_join(stop_words, by = "word")
  

tidy_text %>% 
  count(word) %>% 
  filter(!(word %in% c("boulder", "council", "city", 'fw', "fwd", 2019))) %>% # remove some meaningless words
  with(wordcloud(word, n, max.words = 50))


tidy_phrases <- 
  data_clean %>% 
  unnest_tokens(word, PlainTextBody, token = "ngrams", n = 2) %>% 
  separate(word, into = c("w_1", "w_2"), remove = F) %>% 
  anti_join(stop_words, by = c("w_1" = "word")) %>% 
  anti_join(stop_words, by = c("w_2" = "word")) %>% 
  count(word) %>% 
  arrange(desc(n))

tidy_phrases
```


# Data Analysis
```{r}
non_topics <- c("tinyurl.com", "http", "https", "email", "community", "dear", "city", "boulder", "council", "2", "2017", "2018", "2019")

# create DTM with the cleaned data
clean_dtm <- data_clean %>% 
  unnest_tokens(output = tokenized_emails,
                input  = PlainTextBody) %>% 
  anti_join(stop_words, by = c("tokenized_emails" = "word")) %>% 
  filter(!tokenized_emails %in% non_topics) %>% 
  count(MessageIdentifier, tokenized_emails, sort = T) %>% 
  cast_dtm(term = tokenized_emails, document = MessageIdentifier, value = n)


k <- 5 #number of topics
seed = 1234 #necessary for reproducibility
#fit the model passing the parameters discussed above
#you could have more control parameters but will just use seed here
lda <- LDA(clean_dtm, 
           k = k, 
           method = "GIBBS", 
           control = list(seed = seed))



num_words <- 10 #number of words to visualize

#create function that accepts the lda model and num word to display
top_terms_per_topic <- function(lda_model, num_words) {

  #tidy LDA object to get word, topic, and probability (beta)
  topics_tidy <- tidy(lda_model, matrix = "beta")


  top_terms <- topics_tidy %>%
  group_by(topic) %>%
  arrange(topic, desc(beta)) %>%
  #get the top num_words PER topic
  slice(seq_len(num_words)) %>%
  arrange(topic, beta) %>%
  #row is required for the word_chart() function
  mutate(row = row_number()) %>%
  ungroup() %>%
  #add the word Topic to the topic labels
  mutate(topic = paste("Topic", topic, sep = " "))
  #create a title to pass to word_chart
  title <- paste("LDA Top Terms for", k, "Topics")
  #call the word_chart function you built in prep work
  word_chart(top_terms, top_terms$term, title)
}
#
top_terms_per_topic(lda, num_words)
```

## Open Space

## Bike Trails

## Vaping

## Boulder Homeless Shelter

## 5G Network

